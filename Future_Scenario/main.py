# -*- coding: utf-8 -*-
"""
Created on Thu Nov 23 14:47:35 2023

@author: asalehi
"""


'''

Please Do not Edit this file on Share Point
Please Do not Edit this file on Share Point
Please Do not Edit this file on Share Point
Please Do not Edit this file on Share Point
Please Do not Edit this file on Share Point
Please Do not Edit this file on Share Point
Please Do not Edit this file on Share Point
Please Do not Edit this file on Share Point
Please Do not Edit this file on Share Point


'''
import os
import random

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt

import numpy as np
import json

from statistics import mean

import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import matplotlib.ticker as mticker

from scipy.stats import truncnorm
# %% Initializing Data
def init_AQ_grids(in_dir='Data/AQgrid.gpkg'):
    """
    Initializes and processes the AQ (Air Quality) Grids from a GeoPackage file.

    The function performs the following steps:
    1. Loads the AQ Grid GeoDataFrame from the specified file path.
    2. Converts the coordinate reference system (CRS) to EPSG:4326.
    3. Generates a unique 'GRID_KEY' for each grid based on its 'COL' and 'ROW'.
    4. Filters the GeoDataFrame to keep only the 'GRID_KEY' and 'geometry' columns.

    Parameters:
    - in_dir (str): Path to the input GeoPackage file containing the AQ Grids.
                    Defaults to 'Data/AQgrid.gpkg'.

    Returns:
    - GeoDataFrame: Processed GeoDataFrame containing the AQ Grids with updated CRS,
                    a new 'GRID_KEY', and only the essential columns.
    """
    # Load the AQ Grid GeoDataFrame from the specified GeoPackage file
    AQ_grid_gdf = gpd.read_file(in_dir)
    
    # Convert the GeoDataFrame's coordinate reference system (CRS) to EPSG:4326
    AQ_grid_gdf.crs = "EPSG:4326"
    
    # Generate a 'GRID_KEY' for each grid, combining 'COL' and 'ROW' values
    AQ_grid_gdf['GRID_KEY'] = [str((col, row)) for col, row in zip(AQ_grid_gdf['COL'], AQ_grid_gdf['ROW'])]
    
    # Define the columns to keep in the final GeoDataFrame
    columns_to_keep = ['GRID_KEY', 'geometry']
    
    # Filter the GeoDataFrame to include only the specified columns
    AQ_grid_gdf = AQ_grid_gdf[columns_to_keep]
    
    # Return the processed GeoDataFrame
    return AQ_grid_gdf

def init_climatezone_grid_map(in_dir='Data/AQ_grid_climatezone_IECC2015.gpkg'):
    """
    Initialize a mapping between grid keys and climate zones from a GeoPackage file.

    This function reads a geospatial data file to create a dictionary where each grid key
    corresponds to an International Energy Conservation Code (IECC) climate zone.

    Parameters:
    - in_dir (str): Path to the input GeoPackage file containing the grid key and IECC climate zone data.

    Returns:
    - dict: A dictionary mapping each grid key to its corresponding IECC climate zone.
    """
    
    # Read the GeoPackage file containing the climate zone data
    climatezone_grid_gdf = gpd.read_file(in_dir)
    
    # Initialize a dictionary to store the grid key to climate zone mapping
    climatezone_grid_map = {}
    
    # Iterate over each row in the geodataframe to populate the mapping dictionary
    for idx, row in climatezone_grid_gdf.iterrows():
        GRID_KEY = row['GRID_KEY']  # Extract the grid key
        IECC = row['IECC15']        # Extract the IECC climate zone
        climatezone_grid_map[GRID_KEY] = IECC  # Assign the IECC zone to the grid key in the dictionary
    
    # Return the completed mapping dictionary
    return climatezone_grid_map

def init_County(in_dir='Data/County_Main_Land.gpkg'):
    """
    Initializes and processes the County geometries from a GeoPackage file.

    The function performs the following steps:
    1. Loads the County GeoDataFrame from the specified file path.
    2. Converts the coordinate reference system (CRS) to EPSG:4326.
    3. Filters the GeoDataFrame to keep only the 'FIPS' and 'geometry' columns.

    Parameters:
    - in_dir (str): Path to the input GeoPackage file containing County geometries.
                    Defaults to 'Data/County_Main_Land.gpkg'.

    Returns:
    - GeoDataFrame: Processed GeoDataFrame containing the County geometries with updated CRS,
                    including only the 'FIPS' code and geometry for each county.
    """
    # Load the County GeoDataFrame from the specified GeoPackage file
    County_gdf = gpd.read_file(in_dir)
    
    # Convert the GeoDataFrame's coordinate reference system (CRS) to EPSG:4326
    County_gdf.crs = "EPSG:4326"
    
    # Define the columns to keep in the final GeoDataFrame
    columns_to_keep = ['FIPS', 'geometry']
    
    # Filter the GeoDataFrame to include only the specified columns
    County_gdf = County_gdf[columns_to_keep]
    
    # Return the processed GeoDataFrame
    return County_gdf


def init_grid_contents_in_county_overlap_mapping(  in_dir = 'Data/Spatial_Grid_contents_in_County_Overlap_Percentages.json' ):
    #This function loads a pre calculated dictionary to map percentage of each county area located in each grid
    with open(in_dir, 'r') as file:
        grid_contents_in_county_overlap_mapping = json.load(file)
    
    return grid_contents_in_county_overlap_mapping

def init_county_contents_in_grid_overlap_mapping(  in_dir = 'Data/Spatial_County_contents_in_Grid_Overlap_Percentages.json' ):
    #This function loads a pre calculated dictionary to map percentage of county contents of a grid
    
    with open(in_dir, 'r') as file:
        county_contents_in_grid_overlap_mapping = json.load(file)

    return county_contents_in_grid_overlap_mapping

def init_PM_Concentrations_means(in_dir = 'Data/PM_Grid_Daily_Mean_Concentration_1981_2010.json'):
    # This Function loads PM2.5 Concentration per Grid Cell per year, and returns the yearly mean value
    with open(in_dir, 'r') as file:
        PM_concentrations_mean_dict = json.load(file)
    
    return PM_concentrations_mean_dict

def init_future_PM_Concentrations_means(scenario = 'REF', init_condition = 'IC1', in_dir = 'Data/'):
    # This Function loads PM2.5 Concentration per Grid Cell per year, and returns the yearly mean value
    in_dir_file = in_dir + scenario + '_PM25_2050_2100_d.json'
    with open(in_dir_file, 'r') as file:
        PM_concentrations_mean_dict = json.load(file)
    
    return PM_concentrations_mean_dict
    #PM_concentrations_mean_ic_dict = PM_concentrations_mean_dict[init_condition]
    #return PM_concentrations_mean_ic_dict

def start_centuryPM():
    
    PM_dict = init_PM_Concentrations_means()
    # Initialize the new dictionary to hold the reorganized structure
    reorganized_dict = {}
    start_centuryPM_dict = {}
    # Iterate through each level of the nested dictionary
    for year, grid_dict in PM_dict.items():
        for g, value in grid_dict.items():
        
                if g not in reorganized_dict:
                    reorganized_dict[g] = {}
                # Assign the value to the new key structure
                reorganized_dict[g][year] = value
    
    # Return the reorganized dictionary
    for g in reorganized_dict.keys():
        PM_grid = reorganized_dict[g]
        PM_list = []
        for year in PM_grid.keys():
            PM_list.append(PM_grid[year])
            
        PM_average = mean(PM_list)
        start_centuryPM_dict[g] = PM_average
        
    return start_centuryPM_dict
        


def init_Buildings_Stock( in_dir = 'Data/Buildings_Stock.csv'):
    #This Function Loads the Buildings Stock
    Buildings_Stock_df = pd.read_csv(in_dir)
    return Buildings_Stock_df

def init_ACH50(in_dir = 'Data/ACH50_Grid_Vintage_BT.json'):
    #This function initilizes ACH50 across grid cells per building type
    #For each grid and building type 1000 mean ACH50 occupant based values is provided
    with open(in_dir, 'r') as file:
        ACH50_dict = json.load(file)
    return ACH50_dict


def init_ACH50_national_mean(in_dir = 'Data/ACH50_national_mean.json'):
    #This function initilizes ACH50 across grid cells per building type
    #For each grid and building type 1000 mean ACH50 occupant based values is provided
    with open(in_dir, 'r') as file:
        ACH50_national_mean = json.load(file)
    return ACH50_national_mean

def init_floor_area(in_dir = 'Data/Floorarea_Grid_Vintage_BT.json'):
    #This function initilizes Floor area across grid cells per building type
    
    #This function initilizes ACH50 across grid cells per building type
    #For each grid and building type 1000 mean ACH50 occupant based values is provided
    with open(in_dir, 'r') as file:
        fl_area_dict = json.load(file)
    return fl_area_dict

def init_floor_area_national_mean(in_dir = 'Data/Floorarea_national_mean.json'):
    #This function initilizes ACH50 across grid cells per building type
    #For each grid and building type 1000 mean ACH50 occupant based values is provided
    with open(in_dir, 'r') as file:
        fl_area_national_mean = json.load(file)
    return fl_area_national_mean

def init_occupants(in_dir = 'Data/Occupants_Grid_Vintage_BT.json'):
    #This function initilizes Floor area across grid cells per building type
    
    #This function initilizes ACH50 across grid cells per building type
    #For each grid and building type 1000 mean ACH50 occupant based values is provided
    with open(in_dir, 'r') as file:
        occupants_dict = json.load(file)
    return occupants_dict

def init_occupants_national_mean(in_dir = 'Data/Occupants_national_mean.json'):
    #This function initilizes ACH50 across grid cells per building type
    #For each grid and building type 1000 mean ACH50 occupant based values is provided
    with open(in_dir, 'r') as file:
        occupants_national_mean = json.load(file)
    return occupants_national_mean

test = init_occupants_national_mean()

def reorganized_dict(target_dict):
    """
    Reorganize a nested dictionary to change the hierarchy of the keys.

    This function takes a dictionary with a nested structure and reorganizes it
    such that the innermost keys become the outermost keys in the resulting dictionary.

    Parameters:
    - target_dict (dict): A nested dictionary to reorganize. It's assumed to have
                          a structure where the first level of keys contains values
                          that are also dictionaries, and so on.

    Returns:
    - dict: A reorganized dictionary with the innermost keys as the outermost keys.
    
    Example Structure:
    - Before: target_dict[v][bt][g] -> value
    - After: reorganized_dict[g][v][bt] -> value
    """
    # Initialize the new dictionary to hold the reorganized structure
    reorganized_dict = {}
    
    # Iterate through each level of the nested dictionary
    for v, bt_dict in target_dict.items():
        for bt, g_dict in bt_dict.items():
            for g, value in g_dict.items():
                # Create a new hierarchy with the inner key as the primary key
                if g not in reorganized_dict:
                    reorganized_dict[g] = {}
                if v not in reorganized_dict[g]:
                    reorganized_dict[g][v] = {}
                # Assign the value to the new key structure
                reorganized_dict[g][v][bt] = value
    
    # Return the reorganized dictionary
    return reorganized_dict

def init_ach50_mean(in_dir = 'Data/ACH50_Grid_Vintage_BT.json'):
    #Function Only for deterministic model Not applicable for MCS
    with open(in_dir, 'r') as file:
        ach50 = json.load(file)
    
    national_average = init_ACH50_national_mean()

    ach50_mean = {}
    for v in ach50.keys():
        ach50_mean[v] = {}
        for bt in ach50[v].keys():
            ach50_mean[v][bt] = {}
            for g in ach50[v][bt].keys():
                ach50_mean[v][bt][g] = sum(ach50[v][bt][g]) / len(ach50[v][bt][g])
                if pd.isna(ach50_mean[v][bt][g]):
                    segment = bt + '_' + v
                    ach50_mean[v][bt][g] = national_average[segment]
                
    ach50_mean = reorganized_dict(ach50_mean)
    return ach50_mean

test = init_ach50_mean()

def sample_ach50_mean( ach50 ):
    #Function for MCS
    national_average = init_ACH50_national_mean()

    ach50_mean = {}
    for v in ach50.keys():
        ach50_mean[v] = {}
        for bt in ach50[v].keys():
            ach50_mean[v][bt] = {}
            for g in ach50[v][bt].keys():
                
                ach50_mean[v][bt][g] = random.choice(ach50[v][bt][g])
                
                if pd.isna(ach50_mean[v][bt][g]):
                    segment = bt + '_' + v
                    ach50_mean[v][bt][g] = national_average[segment]
                
    ach50_mean = reorganized_dict(ach50_mean)
    return ach50_mean

def sample_flarea_mean( flarea ):
    #Function for MCS
    national_average = init_floor_area_national_mean()

    flarea_mean = {}
    for v in flarea.keys():
        flarea_mean[v] = {}
        for bt in flarea[v].keys():
            flarea_mean[v][bt] = {}
            for g in flarea[v][bt].keys():
                
                flarea_mean[v][bt][g] = random.choice(flarea[v][bt][g])
                
                if pd.isna(flarea_mean[v][bt][g]):
                    segment = bt + '_' + v
                    flarea_mean[v][bt][g] = national_average[segment]
                
    flarea_mean = reorganized_dict(flarea_mean)
    return flarea_mean

def sample_occupants_mean( occupants ):
    #Function for MCS
    national_average = init_occupants_national_mean()

    occupants_mean = {}
    for v in occupants.keys():
        occupants_mean[v] = {}
        for bt in occupants[v].keys():
            occupants_mean[v][bt] = {}
            for g in occupants[v][bt].keys():
                
                occupants_mean[v][bt][g] = random.choice(occupants[v][bt][g])
                
                if pd.isna(occupants_mean[v][bt][g]):
                    segment = bt + '_' + v
                    occupants_mean[v][bt][g] = national_average[segment]
                
    occupants_mean = reorganized_dict(occupants_mean)
    return occupants_mean


def init_population(ref_year, in_dir='Data/Population.csv'):
    """
    Initialize population data based on a reference year and return it as a dictionary.

    The current model uses population data for a specific reference year (default is 2000).
    The data is organized into a dictionary where keys are grid cell coordinates (COL, ROW) 
    and values are the population in that grid cell for the reference year.

    Parameters:
    - ref_year (int, optional): The reference year for which the population data is extracted. Default is 2000.
    - in_dir (str, optional): The file path to the CSV file containing population data. Default is 'Data/Population.csv'.

    Returns:
    - dict: A dictionary where keys are grid cell coordinates as strings and values are the population for that cell.
    """
    
    # Read population data from the specified CSV file into a DataFrame
    pop_df = pd.read_csv(in_dir)
    
    # Initialize an empty dictionary to store population data
    pop_dict = {}
    
    # Convert the reference year to a string for column access
    ref_year = str(ref_year)
    
    # Iterate over each row in the DataFrame to populate the population dictionary
    for index, row in pop_df.iterrows():
        population = row[ref_year]  # Population for the given reference year
        col_row_key = ( int(row['COL']), int(row['ROW']))  # Tuple key for grid cell coordinates (COL, ROW)
        grid_key = str(col_row_key)  # Convert tuple to string to create a unique dictionary key
        
        # Store population data in the dictionary with the grid cell as the key
        pop_dict[grid_key] = population
        
    # Return the dictionary containing population data for each grid cell
    return pop_dict

def init_baseline_mortality(ref_year=2000, in_dir='Data/Baseline_Mortality.csv'):
    """
    Initialize baseline mortality data for a specified reference year and organize it into a dictionary.

    This function reads a CSV file containing baseline mortality rates for various grid cells (COL, ROW) and 
    returns the data as a dictionary. Each key in the dictionary is a unique identifier for a grid cell, 
    and the corresponding value is the baseline mortality rate for the specified reference year.

    Parameters:
    - ref_year (int, optional): The reference year for which the baseline mortality data is extracted. Default is 2000.
    - in_dir (str, optional): The file path to the CSV file containing baseline mortality data. Default is 'Data/Baseline_Mortality.csv'.

    Returns:
    - dict: A dictionary where keys are grid cell coordinates as strings and values are the baseline mortality rate 
            for that grid cell in the specified reference year.
    """
    
    # Read the baseline mortality data from the specified CSV file into a DataFrame
    baseline_mortality_df = pd.read_csv(in_dir)
    
    # Initialize an empty dictionary to store baseline mortality data
    baseline_mortality_dict = {}
    
    # Convert the reference year to a string to access the corresponding column in the DataFrame
    ref_year = str(ref_year)
    
    # Iterate through each row in the DataFrame to populate the baseline mortality dictionary
    for index, row in baseline_mortality_df.iterrows():
        baseline_mortality = row[ref_year]  # Baseline mortality for the given reference year
        col_row_key = (int(row['COL']), int(row['ROW']))  # Tuple representing grid cell coordinates (COL, ROW)
        grid_key = str(col_row_key)  # Convert the tuple to a string to create a unique dictionary key
        
        # Store the baseline mortality rate in the dictionary with the grid cell as the key
        baseline_mortality_dict[grid_key] = baseline_mortality
        
    # Return the dictionary containing baseline mortality data for each grid cell
    return baseline_mortality_dict



def adjust_vsl( ref_income, target_income, ref_vsl, elacticity = 0.4):
    
    #epsilon based on US EPA practice
    target_vsl = ref_vsl * ((target_income / ref_income)**elacticity)
    
    return target_vsl

def init_VSL():
    
    #epsilon based on US EPA practice
    #target_vsl = ref_vsl * ((ref_income / target_income)**elacticity)
    
    #return target_vsl

    return 11420000
    #return 7000000

def init_Health_Model():
    #This values need to be Corrected
    Baseline_Mortality_dict = init_baseline_mortality()
    
    Relative_Risk = 1.14
    VSL = init_VSL()
    
    return VSL, Relative_Risk, Baseline_Mortality_dict

def init_segment_mapping():
    
    segment_maping = { '<1940' : { 'Multi-Family with 5+ Units': 'MF5P_V1'
                    , 'Multi-Family with 2 - 4 Units': 'MF24_V1'
                    , 'Single-Family Attached': 'SF_V1'
                    , 'Single-Family Detached': 'SF_V1'
                    , 'Mobile Home': 'MH_V1' },
                    
                    '1940s' : { 'Multi-Family with 5+ Units': 'MF5P_V2'
                    , 'Multi-Family with 2 - 4 Units': 'MF24_V2'
                    , 'Single-Family Attached': 'SF_V2'
                    , 'Single-Family Detached': 'SF_V2'
                    , 'Mobile Home': 'MH_V2' },
                    
                    '1950s' : { 'Multi-Family with 5+ Units': 'MF5P_V2'
                    , 'Multi-Family with 2 - 4 Units': 'MF24_V2'
                    , 'Single-Family Attached': 'SF_V2'
                    , 'Single-Family Detached': 'SF_V2'
                    , 'Mobile Home': 'MH_V2' },
                    
                    '1960s' : { 'Multi-Family with 5+ Units': 'MF5P_V2'
                    , 'Multi-Family with 2 - 4 Units': 'MF24_V2'
                    , 'Single-Family Attached': 'SF_V2'
                    , 'Single-Family Detached': 'SF_V2'
                    , 'Mobile Home': 'MH_V2' },
                    
                    '1970s' : { 'Multi-Family with 5+ Units': 'MF5P_V2'
                    , 'Multi-Family with 2 - 4 Units': 'MF24_V2'
                    , 'Single-Family Attached': 'SF_V2'
                    , 'Single-Family Detached': 'SF_V2'
                    , 'Mobile Home': 'MH_V2' },
                    
                    '1980s' : { 'Multi-Family with 5+ Units': 'MF5P_V3'
                    , 'Multi-Family with 2 - 4 Units': 'MF24_V3'
                    , 'Single-Family Attached': 'SF_V3'
                    , 'Single-Family Detached': 'SF_V3'
                    , 'Mobile Home': 'MH_V3' },
                    
                    '1990s' : { 'Multi-Family with 5+ Units': 'MF5P_V3'
                    , 'Multi-Family with 2 - 4 Units': 'MF24_V3'
                    , 'Single-Family Attached': 'SF_V3'
                    , 'Single-Family Detached': 'SF_V3'
                    , 'Mobile Home': 'MH_V3' },
                    
                    '2000s' : { 'Multi-Family with 5+ Units': 'MF5P_V3'
                    , 'Multi-Family with 2 - 4 Units': 'MF24_V3'
                    , 'Single-Family Attached': 'SF_V3'
                    , 'Single-Family Detached': 'SF_V3'
                    , 'Mobile Home': 'MH_V3' } }
    
    return segment_maping

def segment_matching(target_dict):
    matched_dict = {}
    for v in target_dict.keys():
        for bt in target_dict[v].keys():
            segment_key = bt + '_' + v
            #print(segment_key)
            matched_dict[segment_key] = target_dict[v][bt]

    return matched_dict
    
def init_phi():
    
    #phi = init_Population_Distribution()
    phi = {
    'SF_V1': 0.1,
    'SF_V2': 0.33,
    'SF_V3': 0.3,
    'MF24_V1': 0.01,
    'MF24_V2': 0.03,
    'MF24_V3': 0.03,
    'MF5P_V1': 0.02,
    'MF5P_V2': 0.06,
    'MF5P_V3': 0.05,
    'MH_V1': 0.01,
    'MH_V2': 0.03,
    'MH_V3': 0.02
    }   

    return phi
# %% Buildingd Part

def ACH50_to_Finf(ach50, P=0.97, K=0.39, F=16):
    """
    Convert blower door test results (ACH_50) to natural infiltration rate (Finf).

    This function estimates the natural infiltration rate based on the air change rate at
    50 Pascals (ACH_50) from blower door tests. It accounts for the penetration factor of
    particulate matter (P), the deposition rate (K), and the conversion factor (F) to
    adjust ACH_50 to ACH_natural.

    Parameters:
    - ACH_50 (float): Air Change per Hour at 50 Pascals from a blower door test.
    - P (float): Penetration factor representing how much particulate matter gets inside. Default is 0.97.
    - K (float): Deposition factor representing how fast particulate matter settles. Default is 0.39.
    - F (float): Conversion factor to relate ACH_50 to natural air change rate. Default is 20.

    Returns:
    - float: The natural infiltration rate (Finf).

    Notes:
    - The user of this function should consider local building codes and literature for appropriate values of P, K, and F.
    - The default values provided may not be suitable for all building types and climates.
    """
    # Convert ACH_50 to natural air change rate (ACH_natural)
    #print(P)
    ach_natural = ach50 / F
    
    # Calculate natural infiltration rate (Finf)
    Finf = (P * ach_natural) / (ach_natural + K)
    
    return Finf, ach_natural


#%% Exposure

def concentration_indoor_calculator(Finf, C_out):
    """
    Calculate the indoor concentration of a pollutant.

    This function computes the indoor concentration of a pollutant based on the
    outdoor concentration and the infiltration factor.

    Parameters:
    - Finf (float): The infiltration factor, representing the fraction of outdoor
                    air pollutants that penetrate indoors.
    - C_out (float): The outdoor concentration of the pollutant.

    Returns:
    - float: The calculated indoor concentration of the pollutant.
    """

    # Calculate the indoor concentration
    C_in = C_out * Finf

    return C_in

def mean_concentration_indoor_calculator(phi, FINF, C_out):
    """
    Calculate the mean indoor concentration of a pollutant, weighted by the
    percentage of people living in different building sections.

    This function computes the mean indoor concentration by considering the
    outdoor concentration, the infiltration factors for different building
    sections, and the distribution of people across these sections.

    Parameters:
    - phi (dict): A dictionary mapping building sections to the percentage of
                  people living in each section. Keys represent section identifiers,
                  and values are the percentage (as a float) of people in each section.
    - FINF (dict): A dictionary mapping building sections to their infiltration
                   factors. Keys represent section identifiers, and values are
                   the infiltration factors (as floats) for each section.
    - C_out (float): The outdoor concentration of the pollutant.

    Returns:
    - float: The mean indoor concentration of the pollutant, weighted by the
             distribution of people across building sections.

    Note: It is assumed that `concentration_indoor_calculator` is a previously
    defined function that calculates the indoor concentration for a given
    infiltration factor and outdoor concentration.
    """
    
    mean_C_in = 0  # Initialize mean indoor concentration
    C_in = {}  # Dictionary to store indoor concentrations for each section
    residences_segment = FINF.keys()  # Building sections
    
    for j in residences_segment:
        Finf = FINF[j]  # Infiltration factor for the section
        C_in[j] = concentration_indoor_calculator(Finf, C_out)  # Calculate indoor concentration
        mean_C_in += C_in[j] * phi[j]  # Weighted sum of indoor concentrations
        
    return mean_C_in


def delta_exposure_calculator(t_in, phi, FINF_baseline, FINF_intervention, C_out):
    """
    Calculate the change in exposure to a pollutant indoors due to an intervention.

    This function computes the difference in mean indoor concentrations of a pollutant
    between a baseline scenario and an intervention scenario, then calculates the 
    change in exposure using time spend indoors.

    Parameters:
    - t_in (float): The time period over which exposure is calculated, in the same units
                    as used for concentration (e.g., hours).
    - phi (dict): A dictionary mapping building sections to the percentage of
                  people living in each section. Keys are section identifiers,
                  and values are percentages of people.
    - FINF_baseline (dict): Infiltration factors for each building section under the
                            baseline scenario. Keys are section identifiers, and values
                            are infiltration factors.
    - FINF_intervention (dict): Infiltration factors for each building section under the
                                intervention scenario. Keys are section identifiers, and
                                values are infiltration factors.
    - C_out (float): The outdoor concentration of the pollutant.

    Returns:
    - Tuple: 
        - delta_mean_C_in (float): The difference in mean indoor concentrations between
                                   the intervention and baseline scenarios.
        - delta_exposure (float): The change in exposure to the pollutant over the
                                  specified time period, calculated using the difference
                                  in mean indoor concentrations.

    Note: Assumes the existence of a `mean_concentration_indoor_calculator` function that
    calculates the mean indoor concentration for given phi, FINF, and C_out parameters.
    """
    
    # Calculate mean indoor concentrations for baseline and intervention scenarios
    mean_C_in_baseline = mean_concentration_indoor_calculator(phi, FINF_baseline, C_out)
    mean_C_in_intervention = mean_concentration_indoor_calculator(phi, FINF_intervention, C_out)
    
    # Compute the difference in mean indoor concentrations due to the intervention
    delta_mean_C_in = mean_C_in_intervention - mean_C_in_baseline
    
    # Calculate the change in exposure over the specified time period
    delta_exposure = t_in * delta_mean_C_in
    
    return delta_mean_C_in, delta_exposure

def sf_concentration_calculator( FINF_baseline, FINF_intervention, C_out):
    """
    Calculate the change in exposure to a pollutant indoors due to an intervention.

    This function computes the difference in mean indoor concentrations of a pollutant
    between a baseline scenario and an intervention scenario, then calculates the 
    change in exposure using time spend indoors.

    Parameters:
    - t_in (float): The time period over which exposure is calculated, in the same units
                    as used for concentration (e.g., hours).
    - phi (dict): A dictionary mapping building sections to the percentage of
                  people living in each section. Keys are section identifiers,
                  and values are percentages of people.
    - FINF_baseline (dict): Infiltration factors for each building section under the
                            baseline scenario. Keys are section identifiers, and values
                            are infiltration factors.
    - FINF_intervention (dict): Infiltration factors for each building section under the
                                intervention scenario. Keys are section identifiers, and
                                values are infiltration factors.
    - C_out (float): The outdoor concentration of the pollutant.

    Returns:
    - Tuple: 
        - delta_mean_C_in (float): The difference in mean indoor concentrations between
                                   the intervention and baseline scenarios.
        - delta_exposure (float): The change in exposure to the pollutant over the
                                  specified time period, calculated using the difference
                                  in mean indoor concentrations.

    Note: Assumes the existence of a `mean_concentration_indoor_calculator` function that
    calculates the mean indoor concentration for given phi, FINF, and C_out parameters.
    """
    
    alt_phi = {
    'SF_V1': 0.137,
    'SF_V2': 0.452,
    'SF_V3': 0.411,
    }   
    
    # Calculate mean indoor concentrations for baseline and intervention scenarios
    mean_C_in_baseline_V1 = FINF_baseline['SF_V1'] * C_out
    mean_C_in_intervention_V1 = FINF_intervention['SF_V1'] * C_out
    
    mean_C_in_baseline_V2 = FINF_baseline['SF_V2'] * C_out
    mean_C_in_intervention_V2 = FINF_intervention['SF_V2'] * C_out
    
    mean_C_in_baseline_V3 = FINF_baseline['SF_V3'] * C_out
    mean_C_in_intervention_V3 = FINF_intervention['SF_V3'] * C_out
    
    # Compute the difference in mean indoor concentrations due to the intervention
    delta_mean_C_in_V1 = mean_C_in_intervention_V1 - mean_C_in_baseline_V1
    delta_mean_C_in_V2 = mean_C_in_intervention_V2 - mean_C_in_baseline_V2
    delta_mean_C_in_V3 = mean_C_in_intervention_V3 - mean_C_in_baseline_V3
    
    delta_mean_C_in = { 'SF_V1' : delta_mean_C_in_V1, 'SF_V2' : delta_mean_C_in_V2, 'SF_V3' : delta_mean_C_in_V3 }
    mean_C_in_baseline = { 'SF_V1' : mean_C_in_baseline_V1, 'SF_V2' : mean_C_in_baseline_V2, 'SF_V3' : mean_C_in_baseline_V3 }
    mean_C_in_intervention = { 'SF_V1' : mean_C_in_intervention_V1, 'SF_V2' : mean_C_in_intervention_V2, 'SF_V3' : mean_C_in_intervention_V3 }
    
    return delta_mean_C_in, mean_C_in_baseline, mean_C_in_intervention

# %% Health Model


def psi_modifier(t_r=0, t_out=0, t_v=0, t_other=0, F_r=0, F_v=0, F_other=0):
    """
    Calculate the exposure modifier (psi) based on the time spent and infiltration factors in various microenvironments.

    The psi modifier represents the relative impact of different environments on an individual's
    overall exposure to outdoor pollutants, given the infiltration factors of those environments
    and the proportion of time spent in each.

    Parameters:
    - t_r (float): Fraction of time spent in residence.
    - t_out (float): Fraction of time spent outdoors.
    - t_v (float): Fraction of time spent in vehicles.
    - t_other (float): Fraction of time spent in other environments.
    - F_r (float): Infiltration factor for residence, the main focus of this study.
    - F_v (float): Infiltration factor for vehicles.
    - F_other (float): Infiltration factor for other environments.

    Returns:
    - float: The exposure modifier (psi) reflecting the weighted impact of time spent and infiltration across environments.

    Note:
    - The psi is dimensionless and provides a way to adjust exposure estimates to reflect time-activity patterns.
    """
    # Calculate the exposure modifier (psi)
    #Should be adjusted further
    #psi = t_r / (t_out + t_v * F_v + t_other * F_other + t_r * F_r)
    #psi = (0.7 / 0.6)
    psi = 1.248
    #psi = 1
    return psi


def HF_1(Y_0, RR, dC_in, psi):
    """
    Calculate the change in risk of mortality based on a linear concentration-response function.

    This function uses the baseline mortality rate (BMR), relative risk (RR),
    change in indoor concentration of a pollutant (dC_in), and a proportionality
    constant (psi) to estimate the change in risk of mortality (dY).

    Parameters:
    - Y_0 (float): Baseline mortality rate (BMR), representing the initial risk of mortality.
    - RR (float): Relative risk associated with exposure to the pollutant.
    - dC_in (float): Change in indoor concentration of the pollutant.
    - psi (float): Proportionality constant linking the change in concentration to the change in risk.

    Returns:
    - float: The estimated change in risk of mortality (dY) due to the change in pollutant concentration.

    Notes:
    - The attributable fraction (AF) is calculated as (RR - 1) / RR.
    - The change in risk of mortality (dY) is calculated as Y_0 * AF * dC_in * psi.
    """
    
    # Calculate the attributable fraction (AF) based on relative risk (RR)
    AF = (RR - 1) / RR
    
    # Calculate the change in risk of mortality (dY)
    dY = Y_0 * AF * (dC_in/10) * psi
    
    return dY


def HF_2(Y_0, beta, dC_in, psi):
    """
    Calculate the change in health outcome using an exponential concentration-response function.

    This function uses the baseline health outcome rate (Y_0), the coefficient beta (β),
    the change in concentration (dC_in), and the exposure duration adjustment (psi),
    to estimate the change in health outcome (ΔY) using an exponential CRF.

    Parameters:
    - Y_0 (float): Baseline health outcome rate.
    - beta (float): The coefficient representing the strength of association between the pollutant
                    concentration change and the health outcome change.
    - dC_in (float): The change in indoor concentration of the pollutant.
    - psi (float): The exposure duration adjustment factor.

    Returns:
    - float: The estimated change in health outcome (ΔY).

    Notes:
    - The equation used is ΔY = Y_0 * (e^(β * ΔC_residence,mean,a * ψ) - 1).
    """
    # Calculate the change in health outcome (ΔY) using the exponential CRF
    dY = Y_0 * (np.exp(beta * dC_in * psi) - 1)
    
    return dY

def HF_3(Y_0, beta, dC_in, psi):
    """
    Calculate the change in health outcome using an exponential concentration-response function.

    This function uses the baseline health outcome rate (Y_0), the coefficient beta (β),
    the change in concentration (dC_in), and the exposure duration adjustment (psi),
    to estimate the change in health outcome (ΔY) using an exponential CRF.

    Parameters:
    - Y_0 (float): Baseline health outcome rate.
    - beta (float): The coefficient representing the strength of association between the pollutant
                    concentration change and the health outcome change.
    - dC_in (float): The change in indoor concentration of the pollutant.
    - psi (float): The exposure duration adjustment factor.

    Returns:
    - float: The estimated change in health outcome (ΔY).

    Notes:
    - The equation used is ΔY = Y_0 * (e^(β * ΔC_residence,mean,a * ψ) - 1).
    """
    # Calculate the change in health outcome (ΔY) using the exponential CRF
    dY = -Y_0 * (np.exp(-beta * dC_in * psi) - 1)
    
    return dY

def delta_mortality_calculator(dY, Pop):
    """
    Calculate the change in the number of mortalities based on the change in risk and population size.

    Parameters:
    - dY (float): The change in risk of mortality.
    - Pop (int/float): The population size affected by the change in risk.

    Returns:
    - float: The change in the number of mortalities (dMort).
    """
    dMort = dY * Pop
    return dMort

def health_benefit_calculator(dMort, VSL):
    """
    Calculate the health benefits in monetary terms based on the change in mortality and the value of a statistical life (VSL).

    Parameters:
    - dMort (float): The change in the number of mortalities.
    - VSL (float): The value of a statistical life, which is the monetary value assigned to a change in the risk of mortality.

    Returns:
    - float: The total health benefits in monetary terms.
    """
    benefits = dMort * VSL
    return benefits

#%%
'''
def adaptation_cost_function(ach_reduction, a = 0.7848, b = 0.1891, al = 0.1416, bl = 0.4895, au = 0.303, bu = 2.0187 ):
    
    if ach_reduction > 0:
        cost_median = a * ach_reduction + b
        cost_min = al * np.log( ach_reduction ) + bl
        cost_max = au * np.exp(bu * ach_reduction)
    else:
        cost_median = 0.000005
        cost_min = 0
        cost_max = 0.00001
    
    return cost_median, cost_min, cost_max


'''

'''
def adaptation_cost_function(ach_reduction, a = 0.0085, b = 0.1693, al = 0.0044, bl = 0.16, au = 0.0104, bu = 0.266 ):
    
    if ach_reduction > 0:
        ach_reduction = ach_reduction * 100
        cost_mean = a * ach_reduction + b
        cost_min = al * ach_reduction + bl
        cost_max = au * ach_reduction + bu
    else:
        cost_mean = 0.000005
        cost_min = 0
        cost_max = 0.00001
    
    return cost_mean, cost_min, cost_max
'''

def adaptation_cost_function(ach_reduction, a = 0.0085, b = 0.1693, al = 0.0044, bl = 0.16, au = 0.0104, bu = 0.266 ):
    
    if ach_reduction > 0:
        ach_reduction = ach_reduction * 100
        cost_median = a * ach_reduction + b
        cost_min = al * ach_reduction + bl
        cost_max = au * ach_reduction + bu
    else:
        cost_median = 0.000005
        cost_min = 0
        cost_max = 0.00001
    
    cost_mode = (cost_min + cost_max)/2
    
    #USD 2019 to USD 2020 (Based on RSmean)
    cost_mode = cost_mode * 1.031
    cost_min = cost_min * 1.031
    cost_max = cost_max * 1.031
    return cost_mode, cost_min, cost_max

def adaptation_cost(ach_baseline, ach_intervention, grid_population, phi, floor_area, occupamcy):

    ach_reduction = ((ach_baseline - ach_intervention) / ach_baseline)
    cost_sqft_mode, cost_sqft_l, cost_sqft_u = adaptation_cost_function( ach_reduction )

    cost_sqft = np.random.triangular(left=cost_sqft_l, mode=cost_sqft_mode, right=cost_sqft_u, size=1)

    #cost = cost_sqft[0] * floor_area * ach_reduction
    if ach_reduction > 0:
        cost = cost_sqft[0] * floor_area
    else:
        cost = 0
    
    #print(ach_reduction, ' :fl: ', floor_area, ' :cost: ', cost)
    cost_individual = cost / occupamcy
    cost_population = cost_individual * grid_population * phi
    return cost_population

def adaptation_cost_function_deter(ach_reduction, a = 0.0085, b = 0.1693, al = 0.0044, bl = 0.16, au = 0.0104, bu = 0.266 ):
    
    if ach_reduction > 0:
        ach_reduction = ach_reduction * 100
        cost_median = a * ach_reduction + b
        cost_min = al * ach_reduction + bl
        cost_max = au * ach_reduction + bu
    else:
        cost_median = 0.000005
        cost_min = 0
        cost_max = 0.00001
    
   # cost_mode = (cost_min + cost_max)/2
    
    #USD 2019 to USD 2020 (Based on RSmean)
    cost_median = cost_median * 1.031
    cost_min = cost_min * 1.031
    cost_max = cost_max * 1.031
    return cost_median, cost_min, cost_max

def adaptation_cost_deter(ach_baseline, ach_intervention, grid_population, phi, floor_area, occupamcy):

    ach_reduction = ((ach_baseline - ach_intervention) / ach_baseline)
    cost_sqft_median, cost_sqft_l, cost_sqft_u = adaptation_cost_function_deter( ach_reduction )

    #cost_sqft = np.random.triangular(left=cost_sqft_l, mode=cost_sqft_mode, right=cost_sqft_u, size=1)
    cost_sqft = cost_sqft_median
    #cost = cost_sqft[0] * floor_area * ach_reduction
    if ach_reduction > 0:
        cost = cost_sqft * floor_area
    else:
        cost = 0
    
    #print(ach_reduction, ' :fl: ', floor_area, ' :cost: ', cost)
    cost_individual = cost / occupamcy
    cost_population = cost_individual * grid_population * phi
    return cost_population

'''
def adaptation_cost(ach_baseline, ach_intervention, grid_population, phi, floor_area, occupamcy):

    ach_reduction = ((ach_baseline - ach_intervention) / ach_baseline)
    cost_sqft_mode, cost_sqft_l, cost_sqft_u = adaptation_cost_function( ach_reduction )
    #cost_sqft_mode = cost_sqft_m
    #cost_sqft_mode = 3*cost_sqft_m - cost_sqft_l - cost_sqft_u
    #print(ach_reduction, cost_sqft_mode, cost_sqft_m, cost_sqft_l, cost_sqft_u)
    #if (cost_sqft_mode < cost_sqft_l) or (cost_sqft_mode > cost_sqft_u):
    #    print("Yes")
    #    cost_sqft_mode = cost_sqft_m
    cost_sqft = np.random.triangular(left=cost_sqft_l, mode=cost_sqft_mode, right=cost_sqft_u, size=1)

    #cost = cost_sqft[0] * floor_area * ach_reduction
    if ach_reduction > 0:
        cost = cost_sqft[0] * floor_area
    else:
        cost = 0
    
    print(ach_reduction, ' :fl: ', floor_area, ' :cost: ', cost)
    cost_individual = cost / occupamcy
    cost_population = cost_individual * grid_population * phi
    return cost_population
    #return cost_individual, cost_population
'''
#%% NPV Part
def PV_Convertor(Interest_Rate, Base_Year, Secondary_Year, Value):
    
    #Interest Rate in percentage : Interest_Rate = 10
    
    Delta_Year = Secondary_Year - Base_Year
    PV = Value / (1 + (Interest_Rate/100)) ** Delta_Year
    return PV


#%% Result analysis functions

def national_average(target_dict, pop_dict):
    """
    Calculate the weighted national average value of a target metric based on population data.

    Parameters:
    - target_dict (dict): A dictionary with groups as keys and target values as values.
    - pop_dict (dict): A dictionary with groups as keys and corresponding population numbers as values.
    
    Returns:
    - float: The calculated national average of the target metric, weighted by the population.

    Notes:
    - It's assumed that both dictionaries have the same groups as keys and that the population data is non-negative.
    """
    
    # Initialize the accumulator for the weighted target values and total population
    national_average_value = 0
    total_pop = 0
    
    # Iterate through the groups to accumulate the weighted target values and total population
    for g in pop_dict.keys():
        # Accumulate the total population
        total_pop += pop_dict[g]
        # Accumulate the weighted target value for the group
        national_average_value += target_dict[g] * pop_dict[g]
    
    # Calculate the weighted national average
    national_average_value /= total_pop
    
    return national_average_value
#%% Statistical Distributions

def sample_truncated_normal(mu, sigma, lower=-np.inf, upper=np.inf):
    """
    Samples a single value from a truncated normal distribution.

    Parameters:
    - mu (float): Mean of the normal distribution.
    - sigma (float): Standard deviation of the normal distribution.
    - lower (float): Lower bound of the truncation, default is -infinity.
    - upper (float): Upper bound of the truncation, default is infinity.

    Returns:
    - float: A single sampled value from the specified truncated normal distribution.
    """
    # Calculate the standardized lower and upper bounds
    a, b = (lower - mu) / sigma, (upper - mu) / sigma

    # Create the truncated normal distribution
    trunc_normal = truncnorm(a, b, loc=mu, scale=sigma)

    # Sample a single value
    sample = trunc_normal.rvs(1)

    return sample[0]  # return the single sampled 


#%% Model Run

def code_comply_all( ach50_segment_dict, GRID_KEY, climatezone_grid_map, P_d = 0.97, K_d = 0.37 ):
    if  (climatezone_grid_map[GRID_KEY] == 1) or (climatezone_grid_map[GRID_KEY] == 2) :
        if ach50_segment_dict['SF_V1'] > 5:
            ach50_segment_dict['SF_V1'] = 5
        if ach50_segment_dict['SF_V2'] > 5:
            ach50_segment_dict['SF_V2'] = 5
        if ach50_segment_dict['SF_V3'] > 5:
            ach50_segment_dict['SF_V3'] = 5
    else:
        if ach50_segment_dict['SF_V1'] > 3:    
            ach50_segment_dict['SF_V1'] = 3
        if ach50_segment_dict['SF_V2'] > 3:
            ach50_segment_dict['SF_V2'] = 3
        if ach50_segment_dict['SF_V3'] > 3:
            ach50_segment_dict['SF_V3'] = 3
    leakage_intervention = {key: ACH50_to_Finf(value, P = P_d, K = K_d) for key, value in ach50_segment_dict.items()}
    Finf_intervention = {key: value[0] for key, value in leakage_intervention.items()}
    ACH_intervention = {key: value[1] for key, value in leakage_intervention.items()}
    
    return Finf_intervention, ACH_intervention

def intervention_calculator(ach50_segment_dict, g, P_d = 0.97, K_d = 0.37):
    ach50_segment_dict['SF_V1'] = 5 
    #print(P_d)
    #Finf_intervention = {key: ACH50_to_Finf(0*value + 5) for key, value in ach50_segment_dict.items()}
    Finf_intervention, ACH_intervention = {key: ACH50_to_Finf(value, P = P_d, K = K_d) for key, value in ach50_segment_dict.items()}
    return Finf_intervention, ACH_intervention

#%%

def run_MCS_Future(init_cond = 'IC1', pm_year = '2000', intervention_function = intervention_calculator, HF = 'HF_3', rr_beta = 0.005826891, rr_beta_sd = 0.000962763, iterations = 10):
    
    
    if  1981 <= int(pm_year) <= 2010:
        pm_dict_all = init_PM_Concentrations_means()
        pm_dict = pm_dict_all[pm_year]
    else:
        pm_dict_all = init_future_PM_Concentrations_means(init_condition = init_cond)
        pm_dict = pm_dict_all[init_cond][pm_year]
        
    ref_year_ranges = [
        (1981, 2010, 2000),
        (2036, 2065, 2050),
        (2085, 2115, 2100)
    ]
    
    # Determine the appropriate reference year
    pop_ref_year = None
    for start_year, end_year, ref_year in ref_year_ranges:
        if start_year <= int(pm_year) <= end_year:
            pop_ref_year = ref_year
            #print(ref_year)
    
    #based on linear gdp from paltsev need to fix
    
    
    #income_dict = { 2000 : 46220 , 2015 : 55775, 2050 : 137640, 2100 : 315350 } #This is income from ref 2015 modified based on GDP paltsev

    income_dict = { 2000 : 42820 , 2015 : 47110, 2050 : 86209, 2100 : 168728 } #Paltsev GDP / population GDP capita
    pop_dict = init_population(ref_year = pop_ref_year)
    
    
    grid_key_list = list(pop_dict.keys())
    climatezone_grid_map = init_climatezone_grid_map()
    
    # Load the distribution
    #ach50_distribution = init_ach50_mean()
    ach50_distribution = init_ACH50()
    flarea_distribution = init_floor_area()
    occupants_distribution = init_occupants()
    
    phi = init_phi()
    t_in = 0.7

    psi = psi_modifier()
    
    _, _, Y_0_dict = init_Health_Model()
    #RR = rr
    #RR_sd = rr_sd
    rr_beta = 0.0070
    rr_beta_sd = 0.0016
    list_delta_mean_C_in = []
    

    list_dMort = []
    list_benefit = []
    list_cost = []
    list_NB = []
    

    
    for n in range(iterations):
        print('iteration: ', n)
        #ach50_dict = reorganized_dict(ach50_dict)
        #floor_area_dict = init_floor_area()
        #floor_area_dict = reorganized_dict(floor_area_dict)
        #occupancy_dict = init_occupancy()
        #occupancy_dict = reorganized_dict(occupancy_dict)
        
        #Sample from the distribution
        ach50_dict = sample_ach50_mean(ach50_distribution)
        #ach50_dict = ach50_distribution
        
        flarea_dict = sample_flarea_mean(flarea_distribution)
        occupants_dict = sample_occupants_mean(occupants_distribution)
        
        delta_mean_C_in = {}
        delta_exposure = {}
        
        dY = {}
        dMort = {}
        benefit = {}
        cost = {}
        agg_cost = {}
        NB = {}
        
        #Azimi sptephens
        #rr_beta = 0.0070
        #rr_beta_sd = 0.0016
        
        
        sample_rr_beta = np.random.normal(rr_beta, rr_beta_sd, 1)


        x = sample_rr_beta[0]
        #x = beta
        shape = 1.509588  # This is k
        scale = 9648168   # This is λ
        VSL = np.random.weibull(shape, 1) * scale
        print(pop_ref_year)
        VSL_ad = adjust_vsl(ref_income = income_dict[2015], target_income = income_dict[pop_ref_year], ref_vsl = VSL[0])
        #VSL_2023 = VSL[0] * 1.23
        #VSL 2020 based on bls.gov CPI calculator 2015 -> 2020 (1 Jan)
        #VSL_2020 = VSL[0] * 1.1
        VSL_2020 = VSL_ad * 1.1
        
        #P_random = sample_truncated_normal(0.72, 0.21, lower=0, upper=1)
        #K_random = sample_truncated_normal(0.39, 0.08, lower=0)
        
        for g in grid_key_list:
            
            ach50_segment_dict = segment_matching( ach50_dict[g]  )
            #floor area
            flarea_segment_dict = segment_matching( flarea_dict[g] )
            #occupancy
            occupants_segment_dict = segment_matching( occupants_dict[g] )
            
            '''
            Reengineering required here/; delta ACH needed for cost calculation
            ACH 0 ->  FINF BASELINE
            ACH
            '''
            #P_random = sample_truncated_normal(0.97, 0.06, lower=0, upper=1)
            P_random = sample_truncated_normal(0.72, 0.21, lower=0, upper=1)
            K_random = sample_truncated_normal(0.39, 0.08, lower=0)
            
            #Fix
            #P_random = 0.72
            #K_random = 0.39
            
            #Dictionary Building segment to a tuple of Finf and ACH Nat
            leakage_baseline = {key: ACH50_to_Finf(value, P = P_random, K = K_random) for key, value in ach50_segment_dict.items()}
            
            #Dictionary of Finf
            FINF_baseline = {key: value[0] for key, value in leakage_baseline.items()}
        
            #Dictionary of ACH nat
            ACH_baseline = {key: value[1] for key, value in leakage_baseline.items()}
            
            #Dictionary of Finf and ACH after intervention
            FINF_intervention, ACH_intervention = intervention_function(ach50_segment_dict, g, climatezone_grid_map, P_d = P_random, K_d = K_random)
            
            cost[g] = {key: adaptation_cost(ACH_baseline[key], ACH_intervention[key], pop_dict[g], phi[key],flarea_segment_dict[key] , occupants_segment_dict[key] ) for key, value in ACH_baseline.items()} 

            C_out = pm_dict[g]
            
            delta_mean_C_in[g], delta_exposure[g] = delta_exposure_calculator(t_in, phi, FINF_baseline, FINF_intervention, C_out)

            #beta = 0.0058
            #beta_sd = 0.001
            #sample_beta = np.random.normal(beta, beta_sd)
            #print(sample_beta)

            #x = sample_beta
            if HF == 'HF_1':
                dY[g] = HF_1(Y_0_dict[g], x, -delta_mean_C_in[g], psi)
            elif HF == 'HF_2':
                dY[g] = HF_2(Y_0_dict[g], x, -delta_mean_C_in[g], psi)
            elif HF == 'HF_3':
                #print(x)
                dY[g] = HF_3(Y_0_dict[g], x, -delta_mean_C_in[g], psi)
                
            dMort[g] = delta_mortality_calculator(dY[g], pop_dict[g])
            
        
            
            benefit[g] = health_benefit_calculator(dMort[g], VSL_2020)
            
            agg_cost[g] = sum(cost[g].values())
            NB[g] = benefit[g] - agg_cost[g]
            

        list_delta_mean_C_in.append(delta_mean_C_in)

        list_dMort.append(dMort)
        list_benefit.append(benefit)
        list_cost.append(agg_cost)
        list_NB.append(NB)

    return list_dMort, list_benefit, list_cost, list_NB

def run_all_points( years_list, initial_conditions = [ 'IC1', 'IC2', 'IC3', 'IC4', 'IC5'], n = 10):
    
    s=0
    benefits = {}
    for ic in initial_conditions:
        for year in years_list:
            key = (ic, year)
            s+=1
            print(s)
            _, list_ben, _, _ = run_MCS_Future(init_cond = ic, pm_year = year, intervention_function= code_comply_all, iterations=n)
            benefits[key] = list_ben
    
    return benefits

years_2000 = list(range(1981, 2011))  # range end is exclusive, so add 1
years_2000 = [str(year) for year in years_2000]
years_2050 = list(range(2036, 2066))  # range end is exclusive, so add 1
years_2050 = [str(year) for year in years_2050]
years_2100 = list(range(2086, 2116))  # range end is exclusive, so add 1
years_2100 = [str(year) for year in years_2100]

test0 = run_all_points( years_list = years_2000, initial_conditions = ['HIST'], n = 100)
test = run_all_points(years_list = years_2050, n = 100)
test2 = run_all_points(years_list = years_2100, n = 100)

pm_dict_hist = init_PM_Concentrations_means()
pm_dict = start_centuryPM()
pm_dict_fut = init_future_PM_Concentrations_means()
pop_dict = init_population(ref_year = 2050)
ach50_distribution = init_ACH50()


def match_ben_PM( benefits, pm_dict, year):
    pop_dict = init_population(ref_year = year)
    total_pop = sum(pop_dict.values())
    print(total_pop)
    matched_ben = []
    for key in benefits.keys():
        if year == 2000:
            pm = pm_dict[key[1]]
            pm_avg = national_average(pm, pop_dict)
            
        else:
            pm = pm_dict[key[0]][key[1]]
            pm_avg = national_average(pm, pop_dict)
        
        ben = benefits[key]
        #ben = [ national_average(x, pop_dict) for x in ben]
        ben = [ sum(x.values()) for x in ben]
        ben_avg = sum(ben) / len(ben)
        ben_capita = ben_avg / total_pop
        
        matched_tupple = (pm_avg, ben_capita)
        matched_ben.append(matched_tupple)
        
    return matched_ben

l0 = match_ben_PM(test0, pm_dict_hist, 2000)
l1 = match_ben_PM(test, pm_dict_fut, 2050)
l2 = match_ben_PM(test2, pm_dict_fut, 2100)

def plot_scatter_comparison(data_lists, labels, colors, plot_title='', x_label=r'Ambient $PM_{2.5}$', y_label='Benefits'):
    """
    Plots a scatter diagram comparing multiple data sets with benefits on the y-axis and PM2.5 on the x-axis.
    Adds median lines for each data subset.

    Parameters:
    - data_lists: List of lists, where each list contains tuples of (benefit, PM2.5).
    - labels: List of strings, labels for each data list for the legend.
    - colors: List of strings, colors for each data list in the plot.
    - plot_title: String, title of the plot.
    - x_label: String, label for the x-axis.
    - y_label: String, label for the y-axis.
    """
    plt.figure(figsize=(10, 8))  # Set the figure size

    # Plot each list of tuples
    for data, label, color in zip(data_lists, labels, colors):
        benefits, pm25 = zip(*data)  # Unpack the list of tuples

        # Calculate medians for each sublist
        median_benefits = np.median(benefits)
        median_pm25 = np.median(pm25)

        # Plot data
        plt.scatter(*zip(*data), color=color, label=label)
        
        # Plot median lines
        print(median_pm25)
        plt.axvline(median_benefits, color=color, linestyle='--', linewidth=2)
        plt.axhline(median_pm25, color=color, linestyle='--', linewidth=2)

    # Setting the labels for axes
    plt.xlabel(x_label)
    plt.ylabel(y_label)

    # Adding title and legend
    plt.title(plot_title)
    legend = plt.legend(title='Scenario', title_fontsize=22, fontsize=22)
    
    plt.xlabel(x_label, fontsize=24)
    plt.ylabel(y_label, fontsize=24)
    plt.title(plot_title, fontsize=24)
    plt.tick_params(axis='both', labelsize=22)
    # Display the plot with grid
    plt.grid(False)
    
    plt.savefig("Figures/future.png", dpi=300)
    plt.show()
    
    
data_lists = [l0, l1, l2]
labels = ['2000', 'REF 2050', 'REF 2100']
colors = ['black', 'brown', 'gray']


plot_scatter_comparison(data_lists, labels, colors, y_label='Benefits per capita (2020 USD)')

climatezone_grid_map = init_climatezone_grid_map()
aq_grid_gdf = init_AQ_grids()
